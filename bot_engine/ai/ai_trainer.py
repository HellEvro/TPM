#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ú–æ–¥—É–ª—å –æ–±—É—á–µ–Ω–∏—è AI —Å–∏—Å—Ç–µ–º—ã

–û–±—É—á–∞–µ—Ç—Å—è –Ω–∞:
1. –ò—Å—Ç–æ—Ä–∏–∏ —Ç—Ä–µ–π–¥–æ–≤ (bot_history.py)
2. –ü–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±–æ—Ç–æ–≤)
3. –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö (—Å–≤–µ—á–∏, –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)
"""

import os
import json
import logging
import pickle
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error
import joblib

logger = logging.getLogger('AI.Trainer')


class AITrainer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è AI –º–æ–¥–µ–ª–µ–π
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞"""
        self.models_dir = 'data/ai/models'
        self.data_dir = 'data/ai'
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.data_dir, exist_ok=True)
        
        # –ú–æ–¥–µ–ª–∏
        self.signal_predictor = None  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤ (LONG/SHORT/WAIT)
        self.profit_predictor = None  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç–∏
        self.scaler = StandardScaler()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏
        self._load_models()
        
        logger.info("‚úÖ AITrainer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
        try:
            signal_model_path = os.path.join(self.models_dir, 'signal_predictor.pkl')
            profit_model_path = os.path.join(self.models_dir, 'profit_predictor.pkl')
            scaler_path = os.path.join(self.models_dir, 'scaler.pkl')
            
            loaded_count = 0
            
            if os.path.exists(signal_model_path):
                self.signal_predictor = joblib.load(signal_model_path)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤: {signal_model_path}")
                loaded_count += 1
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
                metadata_path = os.path.join(self.models_dir, 'signal_predictor_metadata.json')
                if os.path.exists(metadata_path):
                    try:
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                            logger.info(f"   üìä –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞: {metadata.get('saved_at', 'unknown')}")
                    except:
                        pass
            else:
                logger.info("‚ÑπÔ∏è –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (–±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)")
            
            if os.path.exists(profit_model_path):
                self.profit_predictor = joblib.load(profit_model_path)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–∏–±—ã–ª–∏: {profit_model_path}")
                loaded_count += 1
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
                metadata_path = os.path.join(self.models_dir, 'profit_predictor_metadata.json')
                if os.path.exists(metadata_path):
                    try:
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                            logger.info(f"   üìä –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞: {metadata.get('saved_at', 'unknown')}")
                    except:
                        pass
            else:
                logger.info("‚ÑπÔ∏è –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–∏–±—ã–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (–±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)")
            
            if os.path.exists(scaler_path):
                self.scaler = joblib.load(scaler_path)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω scaler: {scaler_path}")
                loaded_count += 1
            else:
                logger.info("‚ÑπÔ∏è Scaler –Ω–µ –Ω–∞–π–¥–µ–Ω (–±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)")
            
            if loaded_count > 0:
                logger.info(f"ü§ñ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {loaded_count}/3 - –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –±–æ—Ç–∞–º–∏!")
            else:
                logger.info("üí° –ú–æ–¥–µ–ª–∏ –µ—â–µ –Ω–µ –æ–±—É—á–µ–Ω—ã - –∑–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            import traceback
            logger.warning(traceback.format_exc())
    
    def _save_models(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª–∏"""
        try:
            signal_model_path = os.path.join(self.models_dir, 'signal_predictor.pkl')
            profit_model_path = os.path.join(self.models_dir, 'profit_predictor.pkl')
            scaler_path = os.path.join(self.models_dir, 'scaler.pkl')
            
            saved_count = 0
            
            if self.signal_predictor:
                joblib.dump(self.signal_predictor, signal_model_path)
                logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤: {signal_model_path}")
                saved_count += 1
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
                metadata_path = os.path.join(self.models_dir, 'signal_predictor_metadata.json')
                metadata = {
                    'model_type': 'RandomForestClassifier',
                    'saved_at': datetime.now().isoformat(),
                    'n_estimators': getattr(self.signal_predictor, 'n_estimators', 'unknown'),
                    'max_depth': getattr(self.signal_predictor, 'max_depth', 'unknown')
                }
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            if self.profit_predictor:
                joblib.dump(self.profit_predictor, profit_model_path)
                logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–∏–±—ã–ª–∏: {profit_model_path}")
                saved_count += 1
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
                metadata_path = os.path.join(self.models_dir, 'profit_predictor_metadata.json')
                metadata = {
                    'model_type': 'GradientBoostingRegressor',
                    'saved_at': datetime.now().isoformat(),
                    'n_estimators': getattr(self.profit_predictor, 'n_estimators', 'unknown'),
                    'max_depth': getattr(self.profit_predictor, 'max_depth', 'unknown')
                }
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            if self.scaler:
                joblib.dump(self.scaler, scaler_path)
                logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω scaler: {scaler_path}")
                saved_count += 1
            
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {saved_count}/3")
            logger.info(f"üìÅ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.models_dir}")
            logger.info("ü§ñ –ú–æ–¥–µ–ª–∏ –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –±–æ—Ç–∞–º–∏!")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    def _load_history_data(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏–∏ —Ç—Ä–µ–π–¥–æ–≤"""
        try:
            history_file = os.path.join(self.data_dir, 'history_data.json')
            if not os.path.exists(history_file):
                return []
            
            with open(history_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å–¥–µ–ª–∫–∏ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
            trades = []
            latest = data.get('latest', {})
            history = data.get('history', [])
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–¥–µ–ª–∫–∏ –∏–∑ latest
            if latest:
                trades.extend(latest.get('trades', []))
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–¥–µ–ª–∫–∏ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
            for entry in history:
                trades.extend(entry.get('trades', []))
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–∫—Ä—ã—Ç—ã–µ —Å–¥–µ–ª–∫–∏ —Å PnL
            closed_trades = [
                t for t in trades
                if t.get('status') == 'CLOSED' and t.get('pnl') is not None
            ]
            
            return closed_trades
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—Ä–∏–∏: {e}")
            return []
    
    def _load_market_data(self) -> Dict:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
        - –°–≤–µ—á–∏ –∏–∑ data/candles_cache.json (–Ω–∞–ø—Ä—è–º—É—é –∏–∑ —Ñ–∞–π–ª–∞ - ~554 –º–æ–Ω–µ—Ç—ã, ~554,000 —Å–≤–µ—á–µ–π!)
        - –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏–∑ data/ai/market_data.json –∏–ª–∏ —á–µ—Ä–µ–∑ API
        """
        try:
            market_file = os.path.join(self.data_dir, 'market_data.json')
            
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ market_data.json (–µ—Å–ª–∏ –µ—Å—Ç—å)
            market_data = {}
            if os.path.exists(market_file):
                try:
                    with open(market_file, 'r', encoding='utf-8') as f:
                        market_data = json.load(f)
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è market_data.json: {e}")
            
            # –ï—Å–ª–∏ –Ω–µ—Ç —Å–≤–µ—á–µ–π –≤ market_data, —á–∏—Ç–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é –∏–∑ candles_cache.json
            if not market_data.get('latest', {}).get('candles'):
                logger.info("üìñ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–≤–µ—á–µ–π –Ω–∞–ø—Ä—è–º—É—é –∏–∑ data/candles_cache.json...")
                
                candles_cache_file = os.path.join('data', 'candles_cache.json')
                if os.path.exists(candles_cache_file):
                    try:
                        with open(candles_cache_file, 'r', encoding='utf-8') as f:
                            candles_data = json.load(f)
                        
                        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–≤–µ—á–µ–π –¥–ª—è {len(candles_data)} –º–æ–Ω–µ—Ç –∏–∑ candles_cache.json")
                        
                        if 'latest' not in market_data:
                            market_data['latest'] = {}
                        if 'candles' not in market_data['latest']:
                            market_data['latest']['candles'] = {}
                        
                        candles_count = 0
                        total_candles = 0
                        
                        for symbol, candle_info in candles_data.items():
                            candles = candle_info.get('candles', [])
                            if candles:
                                market_data['latest']['candles'][symbol] = {
                                    'candles': candles,
                                    'timeframe': candle_info.get('timeframe', '6h'),
                                    'last_update': candle_info.get('last_update'),
                                    'count': len(candles),
                                    'source': 'candles_cache.json'
                                }
                                candles_count += 1
                                total_candles += len(candles)
                        
                        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {candles_count} –º–æ–Ω–µ—Ç, {total_candles} —Å–≤–µ—á–µ–π")
                        
                    except json.JSONDecodeError as json_error:
                        logger.warning(f"‚ö†Ô∏è –§–∞–π–ª candles_cache.json –ø–æ–≤—Ä–µ–∂–¥–µ–Ω (JSON –æ—à–∏–±–∫–∞ –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {json_error.pos})")
                        logger.info("üóëÔ∏è –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª, bots.py –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Å—Ç –µ–≥–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
                        try:
                            os.remove(candles_cache_file)
                            logger.info("‚úÖ –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω")
                        except Exception as del_error:
                            logger.debug(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª: {del_error}")
                        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –±–µ–∑ —Å–≤–µ—á–µ–π –∏–∑ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
                    except Exception as e:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è candles_cache.json: {e}")
                        import traceback
                        logger.debug(traceback.format_exc())
            
            return market_data
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {}
    
    def _prepare_features(self, trade: Dict, market_data: Dict = None) -> Optional[np.ndarray]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            trade: –î–∞–Ω–Ω—ã–µ —Å–¥–µ–ª–∫–∏
            market_data: –†—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        Returns:
            –ú–∞—Å—Å–∏–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ None
        """
        try:
            features = []
            
            # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Å–¥–µ–ª–∫–∏
            entry_price = trade.get('entry_price', 0)
            exit_price = trade.get('exit_price', 0)
            direction = trade.get('direction', 'LONG')
            
            if entry_price == 0 or exit_price == 0:
                return None
            
            # –î–∞–Ω–Ω—ã–µ –≤—Ö–æ–¥–∞
            entry_data = trade.get('entry_data', {})
            entry_rsi = entry_data.get('rsi', 50)
            entry_trend = entry_data.get('trend', 'NEUTRAL')
            entry_volatility = entry_data.get('volatility', 0)
            
            # –î–∞–Ω–Ω—ã–µ –≤—ã—Ö–æ–¥–∞
            exit_market_data = trade.get('exit_market_data', {})
            exit_rsi = exit_market_data.get('rsi', 50)
            exit_trend = exit_market_data.get('trend', 'NEUTRAL')
            
            # –ü—Ä–∏–∑–Ω–∞–∫–∏
            features.append(entry_rsi)
            features.append(exit_rsi)
            features.append(entry_volatility)
            features.append(1 if direction == 'LONG' else 0)
            features.append(1 if entry_trend == 'UP' else (0 if entry_trend == 'DOWN' else 0.5))
            features.append(1 if exit_trend == 'UP' else (0 if exit_trend == 'DOWN' else 0.5))
            
            # –ü—Ä–æ—Ü–µ–Ω—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã
            if direction == 'LONG':
                price_change = ((exit_price - entry_price) / entry_price) * 100
            else:
                price_change = ((entry_price - exit_price) / entry_price) * 100
            
            features.append(price_change)
            
            # –í—Ä–µ–º—è –≤ –ø–æ–∑–∏—Ü–∏–∏ (—á–∞—Å—ã)
            entry_time = trade.get('timestamp', '')
            exit_time = trade.get('close_timestamp', '')
            
            if entry_time and exit_time:
                try:
                    entry_dt = datetime.fromisoformat(entry_time.replace('Z', ''))
                    exit_dt = datetime.fromisoformat(exit_time.replace('Z', ''))
                    hours_in_position = (exit_dt - entry_dt).total_seconds() / 3600
                    features.append(hours_in_position)
                except:
                    features.append(0)
            else:
                features.append(0)
            
            return np.array(features)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return None
    
    def train_on_history(self):
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏ —Ç—Ä–µ–π–¥–æ–≤
        """
        logger.info("=" * 80)
        logger.info("üéì –û–ë–£–ß–ï–ù–ò–ï –ù–ê –ò–°–¢–û–†–ò–ò –¢–†–ï–ô–î–û–í")
        logger.info("=" * 80)
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            trades = self._load_history_data()
            
            if len(trades) < 10:
                logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 10, –µ—Å—Ç—å {len(trades)})")
                logger.info("üí° –ù–∞–∫–æ–ø–∏—Ç–µ –±–æ–ª—å—à–µ —Å–¥–µ–ª–æ–∫ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
                return
            
            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(trades)} —Å–¥–µ–ª–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            logger.info(f"üìà –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–¥–µ–ª–∫–∏...")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            X = []
            y_signal = []  # –°–∏–≥–Ω–∞–ª (1 = –ø—Ä–∏–±—ã–ª—å, 0 = —É–±—ã—Ç–æ–∫)
            y_profit = []  # –†–∞–∑–º–µ—Ä –ø—Ä–∏–±—ã–ª–∏/—É–±—ã—Ç–∫–∞
            
            logger.info(f"üîç –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ {len(trades)} —Å–¥–µ–ª–æ–∫...")
            
            processed = 0
            skipped = 0
            
            for trade in trades:
                features = self._prepare_features(trade)
                if features is None:
                    skipped += 1
                    continue
                
                X.append(features)
                
                pnl = trade.get('pnl', 0)
                y_signal.append(1 if pnl > 0 else 0)
                y_profit.append(pnl)
                
                processed += 1
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 20 —Å–¥–µ–ª–æ–∫
                if processed % 20 == 0:
                    logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed}/{len(trades)} —Å–¥–µ–ª–æ–∫...")
            
            if skipped > 0:
                logger.info(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ {skipped} —Å–¥–µ–ª–æ–∫ (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)")
            
            if len(X) < 10:
                logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ({len(X)} –∑–∞–ø–∏—Å–µ–π)")
                return
            
            logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            X = np.array(X)
            y_signal = np.array(y_signal)
            y_profit = np.array(y_profit)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            X_scaled = self.scaler.fit_transform(X)
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
            X_train, X_test, y_signal_train, y_signal_test, y_profit_train, y_profit_test = train_test_split(
                X_scaled, y_signal, y_profit, test_size=0.2, random_state=42
            )
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤
            logger.info("=" * 80)
            logger.info("üéì –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –°–ò–ì–ù–ê–õ–û–í")
            logger.info(f"üìä –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –∑–∞–ø–∏—Å–µ–π")
            logger.info(f"üìä –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} –∑–∞–ø–∏—Å–µ–π")
            logger.info("‚è≥ –û–±—É—á–µ–Ω–∏–µ RandomForestClassifier...")
            
            self.signal_predictor = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            )
            self.signal_predictor.fit(X_train, y_signal_train)
            
            # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å–∏–≥–Ω–∞–ª–æ–≤
            y_signal_pred = self.signal_predictor.predict(X_test)
            accuracy = accuracy_score(y_signal_test, y_signal_pred)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            profitable_pred = sum(y_signal_pred)
            profitable_actual = sum(y_signal_test)
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–∏–≥–Ω–∞–ª–æ–≤ –æ–±—É—á–µ–Ω–∞!")
            logger.info(f"   üìä –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.2%}")
            logger.info(f"   üìà –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö: {profitable_pred}/{len(y_signal_test)}")
            logger.info(f"   üìà –†–µ–∞–ª—å–Ω–æ –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö: {profitable_actual}/{len(y_signal_test)}")
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø—Ä–∏–±—ã–ª–∏
            logger.info("=" * 80)
            logger.info("üéì –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –ü–†–ò–ë–´–õ–ò")
            logger.info("‚è≥ –û–±—É—á–µ–Ω–∏–µ GradientBoostingRegressor...")
            
            self.profit_predictor = GradientBoostingRegressor(
                n_estimators=100,
                max_depth=5,
                random_state=42
            )
            self.profit_predictor.fit(X_train, y_profit_train)
            
            # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–∏–±—ã–ª–∏
            y_profit_pred = self.profit_predictor.predict(X_test)
            mse = mean_squared_error(y_profit_test, y_profit_pred)
            
            avg_profit_actual = np.mean(y_profit_test)
            avg_profit_pred = np.mean(y_profit_pred)
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –ø—Ä–∏–±—ã–ª–∏ –æ–±—É—á–µ–Ω–∞!")
            logger.info(f"   üìä MSE: {mse:.2f}")
            logger.info(f"   üìà –°—Ä–µ–¥–Ω—è—è –ø—Ä–∏–±—ã–ª—å (—Ä–µ–∞–ª—å–Ω–∞—è): {avg_profit_actual:.2f} USDT")
            logger.info(f"   üìà –°—Ä–µ–¥–Ω—è—è –ø—Ä–∏–±—ã–ª—å (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è): {avg_profit_pred:.2f} USDT")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
            self._save_models()
            
            logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏: {e}")
            import traceback
            traceback.print_exc()
    
    def train_on_strategy_params(self):
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø—Ä–∏–≤–æ–¥—è—Ç –∫ –ª—É—á—à–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
        """
        logger.info("üéì –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏...")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            trades = self._load_history_data()
            
            if len(trades) < 10:
                logger.warning("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏")
                return
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            # –ù–∞–ø—Ä–∏–º–µ—Ä, –∫–∞–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è RSI –≤—Ö–æ–¥–∞ –¥–∞—é—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            
            rsi_ranges = {
                'very_low': (0, 25),
                'low': (25, 35),
                'medium': (35, 65),
                'high': (65, 75),
                'very_high': (75, 100)
            }
            
            results = {}
            
            for trade in trades:
                entry_data = trade.get('entry_data', {})
                entry_rsi = entry_data.get('rsi', 50)
                pnl = trade.get('pnl', 0)
                
                for range_name, (low, high) in rsi_ranges.items():
                    if low <= entry_rsi < high:
                        if range_name not in results:
                            results[range_name] = {'trades': 0, 'total_pnl': 0, 'winning': 0}
                        
                        results[range_name]['trades'] += 1
                        results[range_name]['total_pnl'] += pnl
                        if pnl > 0:
                            results[range_name]['winning'] += 1
                        break
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
            analysis_file = os.path.join(self.models_dir, 'strategy_analysis.json')
            with open(analysis_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info("‚úÖ –ê–Ω–∞–ª–∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω")
            logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {json.dumps(results, indent=2, ensure_ascii=False)}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: {e}")
    
    def train_on_historical_data(self):
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö (—Å–≤–µ—á–∞—Ö)
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–≤–µ—á–∏ –∏–∑ data/candles_cache.json –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –≤—Å–µ—Ö –º–æ–Ω–µ—Ç–∞—Ö
        """
        logger.info("=" * 80)
        logger.info("üéì –û–ë–£–ß–ï–ù–ò–ï –ù–ê –ò–°–¢–û–†–ò–ß–ï–°–ö–ò–• –î–ê–ù–ù–´–• (–°–í–ï–ß–ê–•)")
        logger.info("=" * 80)
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Å–≤–µ—á–∏ –∏–∑ candles_cache.json + –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)
            market_data = self._load_market_data()
            
            if not market_data:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return
            
            latest = market_data.get('latest', {})
            candles_data = latest.get('candles', {})
            indicators_data = latest.get('indicators', {})
            
            if not candles_data:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç —Å–≤–µ—á–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ data/candles_cache.json)")
                return
            
            logger.info(f"üìä –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {len(candles_data)} –º–æ–Ω–µ—Ç–∞—Ö —Å–æ —Å–≤–µ—á–∞–º–∏...")
            logger.info(f"üìà –î–æ—Å—Ç—É–ø–Ω–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–ª—è {len(indicators_data)} –º–æ–Ω–µ—Ç")
            
            # –û–±—É—á–∞–µ–º—Å—è –Ω–∞ —Å–≤–µ—á–∞—Ö –∫–∞–∂–¥–æ–π –º–æ–Ω–µ—Ç—ã
            trained_count = 0
            failed_count = 0
            total_candles_processed = 0
            
            for symbol, candle_info in candles_data.items():
                try:
                    candles = candle_info.get('candles', [])
                    if not candles or len(candles) < 50:
                        continue
                    
                    indicators = indicators_data.get(symbol, {})
                    
                    logger.info(f"üéì –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {symbol}:")
                    logger.info(f"   üìä –°–≤–µ—á–µ–π: {len(candles)}")
                    logger.info(f"   üìà RSI: {indicators.get('rsi', 'N/A')}")
                    logger.info(f"   üìà Trend: {indicators.get('trend', 'N/A')}")
                    logger.info(f"   üìà Signal: {indicators.get('signal', 'N/A')}")
                    logger.info(f"   üí∞ Price: {indicators.get('price', 'N/A')}")
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–≤–µ—á–µ–π
                    closes = [float(c.get('close', 0) or c.get('close', 0)) for c in candles]
                    volumes = [float(c.get('volume', 0) or 0) for c in candles]
                    highs = [float(c.get('high', 0) or 0) for c in candles]
                    lows = [float(c.get('low', 0) or 0) for c in candles]
                    opens = [float(c.get('open', 0) or 0) for c in candles]
                    
                    if len(closes) < 50:
                        continue
                    
                    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                    rsi = indicators.get('rsi')
                    trend = indicators.get('trend', 'NEUTRAL')
                    signal = indicators.get('signal', 'WAIT')
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π
                    # –ù–∞–ø—Ä–∏–º–µ—Ä: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ü–µ–Ω–æ–≤—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π, –æ–±—ä–µ–º—ã, –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
                    
                    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
                    if len(closes) > 1:
                        price_changes = [(closes[i] - closes[i-1]) / closes[i-1] * 100 
                                        for i in range(1, len(closes))]
                        volatility = np.std(price_changes) if price_changes else 0
                    else:
                        volatility = 0
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±—ä–µ–º—ã
                    avg_volume = np.mean(volumes) if volumes else 0
                    volume_trend = 'INCREASING' if len(volumes) > 1 and volumes[-1] > volumes[0] else 'DECREASING'
                    
                    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö —Å–≤–µ—á–µ–π
                    # –ù–∞–ø—Ä–∏–º–µ—Ä, –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è—Ö —Ü–µ–Ω–æ–≤—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±—É–¥—É—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–≤–µ—á–∞—Ö
                    
                    trained_count += 1
                    total_candles_processed += len(candles)
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 –º–æ–Ω–µ—Ç
                    if trained_count % 10 == 0:
                        logger.info(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è: {trained_count} –º–æ–Ω–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ, {total_candles_processed} —Å–≤–µ—á–µ–π...")
                    
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ {symbol}: {e}")
                    failed_count += 1
                    continue
            
            logger.info("=" * 80)
            logger.info(f"‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ù–ê –°–í–ï–ß–ê–• –ó–ê–í–ï–†–®–ï–ù–û")
            logger.info(f"   üìä –ú–æ–Ω–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {trained_count}")
            logger.info(f"   üìà –°–≤–µ—á–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_candles_processed}")
            logger.info(f"   ‚ö†Ô∏è –û—à–∏–±–æ–∫: {failed_count}")
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    def predict(self, symbol: str, market_data: Dict) -> Dict:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
        
        Args:
            symbol: –°–∏–º–≤–æ–ª –º–æ–Ω–µ—Ç—ã
            market_data: –†—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (RSI, —Å–≤–µ—á–∏, —Ç—Ä–µ–Ω–¥ –∏ —Ç.–¥.)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º
        """
        if not self.signal_predictor or not self.profit_predictor:
            return {'error': 'Models not trained'}
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ market_data
            features = []
            
            rsi = market_data.get('rsi', 50)
            trend = market_data.get('trend', 'NEUTRAL')
            price = market_data.get('price', 0)
            
            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features.append(rsi)
            features.append(1 if trend == 'UP' else (0 if trend == 'DOWN' else 0.5))
            features.append(price)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω—É–ª–∏ –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–∏–µ)
            while len(features) < 8:
                features.append(0)
            
            features_array = np.array([features])
            features_scaled = self.scaler.transform(features_array)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–∞
            signal_prob = self.signal_predictor.predict_proba(features_scaled)[0]
            predicted_profit = self.profit_predictor.predict(features_scaled)[0]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∏–≥–Ω–∞–ª
            if signal_prob[1] > 0.6:  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–±—ã–ª–∏ > 60%
                signal = 'LONG' if rsi < 35 else 'SHORT' if rsi > 65 else 'WAIT'
            else:
                signal = 'WAIT'
            
            return {
                'signal': signal,
                'confidence': float(signal_prob[1]),
                'predicted_profit': float(predicted_profit),
                'rsi': rsi,
                'trend': trend
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return {'error': str(e)}
    
    def get_trades_count(self) -> int:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        trades = self._load_history_data()
        return len(trades)

