"""
LSTM Predictor –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LSTM –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:
- –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã (–≤–≤–µ—Ä—Ö/–≤–Ω–∏–∑)
- –û–∂–∏–¥–∞–µ–º–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã –≤ %
- –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–æ–≤ –≤ —Å–¥–µ–ª–∫–∏.
"""

import os
import json
import pickle
import logging
import warnings
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import numpy as np
import pandas as pd

try:
    from sklearn.exceptions import NotFittedError
except ImportError:  # pragma: no cover - fallback –µ—Å–ª–∏ scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    class NotFittedError(Exception):
        """–õ–æ–∫–∞–ª—å–Ω—ã–π NotFittedError, –µ—Å–ª–∏ scikit-learn –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        pass

logger = logging.getLogger('LSTM')

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
warnings.filterwarnings('ignore', category=UserWarning, module='keras')

# –û—Ç–∫–ª—é—á–∞–µ–º DEBUG –ª–æ–≥–∏ TensorFlow —á–µ—Ä–µ–∑ Python logging
tensorflow_logger = logging.getLogger('tensorflow')
tensorflow_logger.setLevel(logging.WARNING)
tensorflow_python_logger = logging.getLogger('tensorflow.python')
tensorflow_python_logger.setLevel(logging.WARNING)
tensorflow_core_logger = logging.getLogger('tensorflow.core')
tensorflow_core_logger.setLevel(logging.WARNING)
# –£–¥–∞–ª—è–µ–º –≤—Å–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∏–∑ TensorFlow –ª–æ–≥–≥–µ—Ä–æ–≤
for handler in tensorflow_logger.handlers[:]:
    tensorflow_logger.removeHandler(handler)
for handler in tensorflow_python_logger.handlers[:]:
    tensorflow_python_logger.removeHandler(handler)
for handler in tensorflow_core_logger.handlers[:]:
    tensorflow_core_logger.removeHandler(handler)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å TensorFlow
try:
    import tensorflow as tf
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–∞
    tf.get_logger().setLevel(logging.WARNING)
    from tensorflow import keras
    from tensorflow.keras import layers
    from tensorflow.keras.models import Sequential, load_model
    from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, BatchNormalization
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from sklearn.preprocessing import MinMaxScaler
    TENSORFLOW_AVAILABLE = True
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU –¥–ª—è TensorFlow
    def configure_gpu():
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç TensorFlow –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU
            gpus = tf.config.list_physical_devices('GPU')
            
            if gpus:
                try:
                    # –í–∫–ª—é—á–∞–µ–º —Ä–æ—Å—Ç –ø–∞–º—è—Ç–∏ GPU –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                    for gpu in gpus:
                        tf.config.experimental.set_memory_growth(gpu, True)
                    
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {len(gpus)}")
                    for i, gpu in enumerate(gpus):
                        logger.info(f"   GPU {i}: {gpu.name}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ GPU –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –¥–æ—Å—Ç—É–ø–µ–Ω
                    # –í TensorFlow 2.x GPU –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                    logger.info("‚úÖ GPU –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                    
                    return True
                except RuntimeError as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ GPU: {e}")
                    logger.info("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å CPU...")
                    return False
            else:
                logger.warning("‚ö†Ô∏è GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
                return False
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ GPU: {e}")
            logger.info("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å CPU...")
            return False
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º GPU –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
    GPU_AVAILABLE = configure_gpu()
    
except ImportError:
    TENSORFLOW_AVAILABLE = False
    GPU_AVAILABLE = False
    logger.warning("TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. LSTM Predictor –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")


class LSTMPredictor:
    """
    LSTM –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
    """
    
    def __init__(
        self,
        model_path: str = "data/ai/models/lstm_predictor.keras",  # ‚úÖ Keras 3 —Ñ–æ—Ä–º–∞—Ç
        scaler_path: str = "data/ai/models/lstm_scaler.pkl",
        config_path: str = "data/ai/models/lstm_config.json"
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LSTM –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞
        
        Args:
            model_path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            scaler_path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É scaler'—É
            config_path: –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
        """
        self.model_path = model_path
        self.scaler_path = scaler_path
        self.config_path = config_path
        
        self.model = None
        self.scaler = None
        self.config = {
            'sequence_length': 60,  # 60 —Å–≤–µ—á–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            'features': ['close', 'volume', 'high', 'low', 'rsi', 'ema_fast', 'ema_slow'],
            'prediction_horizon': 6,  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ 6 —á–∞—Å–æ–≤ –≤–ø–µ—Ä–µ–¥ (1 —Å–≤–µ—á–∞)
            'model_version': '1.0',
            'trained_at': None,
            'training_samples': 0
        }
        
        if not TENSORFLOW_AVAILABLE:
            logger.error("TensorFlow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tensorflow")
            return
        
        # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        if TENSORFLOW_AVAILABLE:
            if GPU_AVAILABLE:
                logger.info("üöÄ LSTM Predictor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU")
            else:
                logger.info("üíª LSTM Predictor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (CPU —Ä–µ–∂–∏–º)")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if os.path.exists(model_path) and os.path.exists(scaler_path):
            self.load_model()
        else:
            logger.info("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é")
            self._create_new_model()
    
    def _create_new_model(self):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é LSTM –º–æ–¥–µ–ª—å"""
        if not TENSORFLOW_AVAILABLE:
            return
        
        sequence_length = self.config['sequence_length']
        n_features = len(self.config['features'])
        
        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ LSTM –º–æ–¥–µ–ª–∏ (—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥)
        self.model = Sequential([
            # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
            Input(shape=(sequence_length, n_features)),
            
            # –ü–µ—Ä–≤—ã–π LSTM —Å–ª–æ–π —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            LSTM(128, return_sequences=True),
            Dropout(0.2),
            BatchNormalization(),
            
            # –í—Ç–æ—Ä–æ–π LSTM —Å–ª–æ–π
            LSTM(64, return_sequences=True),
            Dropout(0.2),
            BatchNormalization(),
            
            # –¢—Ä–µ—Ç–∏–π LSTM —Å–ª–æ–π
            LSTM(32, return_sequences=False),
            Dropout(0.2),
            BatchNormalization(),
            
            # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(16, activation='relu'),
            
            # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: 3 –∑–Ω–∞—á–µ–Ω–∏—è [–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, –∏–∑–º–µ–Ω–µ–Ω–∏–µ_%, –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å]
            Dense(3, activation='linear')
        ])
        
        # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
        self.model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        # –°–æ–∑–¥–∞–µ–º scaler
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        
        logger.info("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å")
        logger.info(f"–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {sequence_length} —Å–≤–µ—á–µ–π ‚Üí {n_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    def prepare_features(self, candles: List[Dict]) -> np.ndarray:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Å–≤–µ—á–µ–π –¥–ª—è –º–æ–¥–µ–ª–∏
        
        Args:
            candles: –°–ø–∏—Å–æ–∫ —Å–≤–µ—á–µ–π —Å OHLCV –¥–∞–Ω–Ω—ã–º–∏
        
        Returns:
            –ú–∞—Å—Å–∏–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
        """
        if len(candles) < self.config['sequence_length']:
            logger.debug(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–≤–µ—á–µ–π: {len(candles)} < {self.config['sequence_length']}")
            return None
        
        df = pd.DataFrame(candles)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = []
        for feature in self.config['features']:
            if feature in df.columns:
                features.append(df[feature].values)
            else:
                # –ï—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–µ—Ç, –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                logger.warning(f"–ü—Ä–∏–∑–Ω–∞–∫ {feature} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö")
                features.append(np.zeros(len(df)))
        
        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å (samples, features)
        features = np.array(features).T
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ sequence_length —Å–≤–µ—á–µ–π
        features = features[-self.config['sequence_length']:]
        
        return features.astype(np.float32)
    
    def predict(
        self,
        candles: List[Dict],
        current_price: float
    ) -> Optional[Dict]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –¥–≤–∏–∂–µ–Ω–∏–µ —Ü–µ–Ω—ã
        
        Args:
            candles: –ò—Å—Ç–æ—Ä–∏—è —Å–≤–µ—á–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            current_price: –¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞
        
        Returns:
            {
                'direction': 1 (–≤–≤–µ—Ä—Ö) –∏–ª–∏ -1 (–≤–Ω–∏–∑),
                'change_percent': –æ–∂–∏–¥–∞–µ–º–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤ %,
                'confidence': —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (0-100),
                'predicted_price': –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞,
                'horizon_hours': –≥–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —á–∞—Å–∞—Ö
            }
        """
        if not TENSORFLOW_AVAILABLE or self.model is None:
            return None
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = self.prepare_features(candles)
            if features is None:
                return None
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
            try:
                features_scaled = self.scaler.transform(features)
            except NotFittedError:
                logger.error("Scaler –Ω–µ –æ–±—É—á–µ–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏")
                return None
            except Exception as transform_error:
                logger.error(f"–û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {transform_error}")
                return None
            
            # –î–æ–±–∞–≤–ª—è–µ–º batch dimension
            features_scaled = features_scaled.reshape(1, self.config['sequence_length'], -1).astype(np.float32)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            prediction = self.model.predict(features_scaled, verbose=0)[0]
            
            # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            direction_raw = prediction[0]  # -1 –¥–æ 1
            change_percent = prediction[1]  # % –∏–∑–º–µ–Ω–µ–Ω–∏—è
            confidence = prediction[2]  # 0-1
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            direction = 1 if direction_raw > 0 else -1
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence = min(max(abs(confidence) * 100, 0), 100)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—É—é —Ü–µ–Ω—É
            predicted_price = current_price * (1 + change_percent / 100)
            
            result = {
                'direction': direction,
                'change_percent': float(change_percent),
                'confidence': float(confidence),
                'predicted_price': float(predicted_price),
                'horizon_hours': self.config['prediction_horizon'],
                'current_price': current_price
            }
            
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return None
    
    def train(
        self,
        training_data: List[Tuple[np.ndarray, np.ndarray]],
        validation_split: float = 0.2,
        epochs: int = 50,
        batch_size: int = 32
    ) -> Dict:
        """
        –û–±—É—á–∞–µ—Ç LSTM –º–æ–¥–µ–ª—å
        
        Args:
            training_data: –°–ø–∏—Å–æ–∫ (X, y) –≥–¥–µ X - –ø—Ä–∏–∑–Ω–∞–∫–∏, y - —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        
        Returns:
            –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        """
        if not TENSORFLOW_AVAILABLE or self.model is None:
            return {'error': 'TensorFlow unavailable'}

        if not training_data:
            logger.error("–ü—É—Å—Ç–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return {'error': 'No training data provided'}
        
        try:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            X_list, y_list = zip(*training_data)
            X = np.array(X_list)
            y = np.array(y_list)

            if X.ndim != 3:
                raise ValueError(f"Training data X –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (samples, seq_len, features), –ø–æ–ª—É—á–µ–Ω–æ: {X.shape}")

            if np.isnan(X).any() or np.isinf(X).any():
                logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN/Inf –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–º–µ–Ω—É –Ω–∞ –Ω—É–ª–∏")
                X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if X.shape[-1] != len(self.config['features']):
                logger.warning(
                    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (%s) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π (%s). –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥.",
                    X.shape[-1], len(self.config['features'])
                )
                self.config['features'] = [f'feature_{i}' for i in range(X.shape[-1])]

            # –û–±—É—á–∞–µ–º scaler –Ω–∞ –≤—Å–µ–º –º–∞—Å—Å–∏–≤–µ
            if self.scaler is None:
                self.scaler = MinMaxScaler(feature_range=(0, 1))

            flat_X = X.reshape(-1, X.shape[-1])
            self.scaler.fit(flat_X)
            X_scaled = self.scaler.transform(flat_X).reshape(X.shape).astype(np.float32)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –ª–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
            if TENSORFLOW_AVAILABLE:
                try:
                    import tensorflow as tf
                    gpus = tf.config.list_physical_devices('GPU')
                    if gpus:
                        logger.info(f"üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ GPU: {len(gpus)} —É—Å—Ç—Ä–æ–π—Å—Ç–≤")
                        for i, gpu in enumerate(gpus):
                            logger.info(f"   GPU {i}: {gpu.name}")
                    else:
                        logger.info("üíª –û–±—É—á–µ–Ω–∏–µ –Ω–∞ CPU (GPU –Ω–µ –Ω–∞–π–¥–µ–Ω—ã)")
                except Exception as e:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å GPU: {e}")
            
            logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è: {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤")
            logger.info(f"–§–æ—Ä–º–∞ X: {X.shape}, —Ñ–æ—Ä–º–∞ y: {y.shape}")
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º callbacks
            callbacks = [
                EarlyStopping(
                    monitor='val_loss',
                    patience=10,
                    restore_best_weights=True
                ),
                ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=5,
                    min_lr=0.00001
                )
            ]
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            history = self.model.fit(
                X_scaled, y,
                validation_split=validation_split,
                epochs=epochs,
                batch_size=batch_size,
                callbacks=callbacks,
                verbose=1
            )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            self.config['trained_at'] = datetime.now().isoformat()
            self.config['training_samples'] = len(X)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self.save_model()
            
            logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
            
            return {
                'success': True,
                'final_loss': float(history.history['loss'][-1]),
                'final_val_loss': float(history.history['val_loss'][-1]),
                'epochs_trained': len(history.history['loss']),
                'training_samples': len(X)
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return {'error': str(e)}
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å, scaler –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
        if not TENSORFLOW_AVAILABLE or self.model is None:
            return
        
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self.model.save(self.model_path)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
            with open(self.scaler_path, 'wb') as f:
                pickle.dump(self.scaler, f)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            with open(self.config_path, 'w') as f:
                json.dump(self.config, f, indent=2)
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {self.model_path}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å, scaler –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
        if not TENSORFLOW_AVAILABLE:
            return
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.model = load_model(self.model_path)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º scaler
            with open(self.scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r') as f:
                    self.config.update(json.load(f))
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {self.model_path}")
            logger.info(f"–û–±—É—á–µ–Ω–∞: {self.config.get('trained_at', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            logger.info(f"–û–±—Ä–∞–∑—Ü–æ–≤: {self.config.get('training_samples', 0)}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self._create_new_model()
    
    def get_status(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏"""
        if not TENSORFLOW_AVAILABLE:
            return {
                'available': False,
                'error': 'TensorFlow not installed'
            }
        
        is_trained = (
            self.model is not None and
            os.path.exists(self.model_path) and
            self.config.get('training_samples', 0) > 0
        )
        
        return {
            'available': True,
            'trained': is_trained,
            'model_path': self.model_path,
            'sequence_length': self.config['sequence_length'],
            'prediction_horizon': self.config['prediction_horizon'],
            'trained_at': self.config.get('trained_at'),
            'training_samples': self.config.get('training_samples', 0),
            'features': self.config['features']
        }

