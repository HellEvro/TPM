"""
LSTM Predictor –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LSTM –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:
- –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã (–≤–≤–µ—Ä—Ö/–≤–Ω–∏–∑)
- –û–∂–∏–¥–∞–µ–º–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã –≤ %
- –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏—è

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤—Ö–æ–¥–æ–≤ –≤ —Å–¥–µ–ª–∫–∏.

–¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç PyTorch –≤–º–µ—Å—Ç–æ TensorFlow –¥–ª—è –ª—É—á—à–µ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏ Python 3.14+ –∏ GPU.
"""

import os
import json
import pickle
import logging
import warnings
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import numpy as np
import pandas as pd

try:
    from sklearn.exceptions import NotFittedError
except ImportError:  # pragma: no cover - fallback –µ—Å–ª–∏ scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
    class NotFittedError(Exception):
        """–õ–æ–∫–∞–ª—å–Ω—ã–π NotFittedError, –µ—Å–ª–∏ scikit-learn –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        pass

logger = logging.getLogger('LSTM')

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è PyTorch
warnings.filterwarnings('ignore', category=UserWarning, module='torch')

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å PyTorch
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader, TensorDataset
    from sklearn.preprocessing import MinMaxScaler
    PYTORCH_AVAILABLE = True
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU –¥–ª—è PyTorch
    def configure_gpu():
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç PyTorch –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU NVIDIA (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)"""
        try:
            # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤–µ—Ä—Å–∏–∏ PyTorch
            torch_version = torch.__version__
            logger.debug(f"PyTorch –≤–µ—Ä—Å–∏—è: {torch_version}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                primary_gpu = torch.device('cuda:0')
                gpu_name = torch.cuda.get_device_name(0)
                
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {gpu_count}")
                for i in range(gpu_count):
                    logger.info(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
                
                logger.info(f"‚úÖ GPU NVIDIA –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
                logger.info(f"   –û—Å–Ω–æ–≤–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {gpu_name}")
                
                return True, primary_gpu
            else:
                logger.info("‚ÑπÔ∏è GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
                return False, torch.device('cpu')
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ GPU: {e}")
            logger.info("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å CPU...")
            return False, torch.device('cpu')
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º GPU –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
    GPU_AVAILABLE, DEVICE = configure_gpu()
    
except ImportError:
    PYTORCH_AVAILABLE = False
    GPU_AVAILABLE = False
    DEVICE = None
    logger.warning("PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. LSTM Predictor –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")


# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∞—Å—Å LSTMModel —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ PyTorch –¥–æ—Å—Ç—É–ø–µ–Ω
if PYTORCH_AVAILABLE:
    class LSTMModel(nn.Module):
        """
        PyTorch LSTM –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã
        """
        
        def __init__(self, input_size: int, hidden_sizes: List[int] = [128, 64, 32], dropout: float = 0.2):
            super(LSTMModel, self).__init__()
            
            self.hidden_sizes = hidden_sizes
            self.num_layers = len(hidden_sizes)
            
            # LSTM —Å–ª–æ–∏
            self.lstm1 = nn.LSTM(input_size, hidden_sizes[0], batch_first=True, num_layers=1)
            self.bn1 = nn.BatchNorm1d(hidden_sizes[0])
            self.dropout1 = nn.Dropout(dropout)
            
            self.lstm2 = nn.LSTM(hidden_sizes[0], hidden_sizes[1], batch_first=True, num_layers=1)
            self.bn2 = nn.BatchNorm1d(hidden_sizes[1])
            self.dropout2 = nn.Dropout(dropout)
            
            self.lstm3 = nn.LSTM(hidden_sizes[1], hidden_sizes[2], batch_first=True, num_layers=1)
            self.bn3 = nn.BatchNorm1d(hidden_sizes[2])
            self.dropout3 = nn.Dropout(dropout)
            
            # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
            self.fc1 = nn.Linear(hidden_sizes[2], 32)
            self.dropout4 = nn.Dropout(dropout)
            self.fc2 = nn.Linear(32, 16)
            self.fc3 = nn.Linear(16, 3)  # –í—ã—Ö–æ–¥: [–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, –∏–∑–º–µ–Ω–µ–Ω–∏–µ_%, –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å]
            
        def forward(self, x):
            # –ü–µ—Ä–≤—ã–π LSTM —Å–ª–æ–π (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å)
            lstm_out1, _ = self.lstm1(x)  # (batch, seq_len, hidden1)
            # –ü—Ä–∏–º–µ–Ω—è–µ–º BatchNorm –∫ –∫–∞–∂–¥–æ–º—É –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —à–∞–≥—É
            batch_size, seq_len, hidden = lstm_out1.shape
            lstm_out1 = lstm_out1.reshape(-1, hidden)
            lstm_out1 = self.bn1(lstm_out1)
            lstm_out1 = lstm_out1.reshape(batch_size, seq_len, hidden)
            lstm_out1 = self.dropout1(lstm_out1)
            
            # –í—Ç–æ—Ä–æ–π LSTM —Å–ª–æ–π (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å)
            lstm_out2, _ = self.lstm2(lstm_out1)  # (batch, seq_len, hidden2)
            batch_size, seq_len, hidden = lstm_out2.shape
            lstm_out2 = lstm_out2.reshape(-1, hidden)
            lstm_out2 = self.bn2(lstm_out2)
            lstm_out2 = lstm_out2.reshape(batch_size, seq_len, hidden)
            lstm_out2 = self.dropout2(lstm_out2)
            
            # –¢—Ä–µ—Ç–∏–π LSTM —Å–ª–æ–π (–Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å)
            lstm_out3, _ = self.lstm3(lstm_out2)  # (batch, hidden3)
            lstm_out3 = self.bn3(lstm_out3)
            lstm_out3 = self.dropout3(lstm_out3)
            
            # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
            out = torch.relu(self.fc1(lstm_out3))
            out = self.dropout4(out)
            out = torch.relu(self.fc2(out))
            out = self.fc3(out)  # –õ–∏–Ω–µ–π–Ω—ã–π –≤—ã—Ö–æ–¥
            
            return out
else:
    # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ PyTorch –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    class LSTMModel:
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è LSTMModel –∫–æ–≥–¥–∞ PyTorch –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        def __init__(self, *args, **kwargs):
            raise ImportError("PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch")


class LSTMPredictor:
    """
    LSTM –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç (PyTorch –≤–µ—Ä—Å–∏—è)
    """
    
    def __init__(
        self,
        model_path: str = "data/ai/models/lstm_predictor.pth",  # PyTorch —Ñ–æ—Ä–º–∞—Ç
        scaler_path: str = "data/ai/models/lstm_scaler.pkl",
        config_path: str = "data/ai/models/lstm_config.json"
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LSTM –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞
        
        Args:
            model_path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            scaler_path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É scaler'—É
            config_path: –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
        """
        self.model_path = model_path
        self.scaler_path = scaler_path
        self.config_path = config_path
        
        self.model = None
        self.scaler = None
        self.config = {
            'sequence_length': 60,  # 60 —Å–≤–µ—á–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            'features': ['close', 'volume', 'high', 'low', 'rsi', 'ema_fast', 'ema_slow'],
            'prediction_horizon': 6,  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ 6 —á–∞—Å–æ–≤ –≤–ø–µ—Ä–µ–¥ (1 —Å–≤–µ—á–∞)
            'model_version': '2.0',  # –í–µ—Ä—Å–∏—è 2.0 –¥–ª—è PyTorch
            'trained_at': None,
            'training_samples': 0
        }
        
        if not PYTORCH_AVAILABLE:
            logger.error("PyTorch –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch")
            return
        
        # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        if PYTORCH_AVAILABLE:
            if GPU_AVAILABLE and DEVICE:
                logger.info(f"üöÄ LSTM Predictor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU NVIDIA: {DEVICE}")
            else:
                logger.info("üíª LSTM Predictor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (CPU —Ä–µ–∂–∏–º)")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if os.path.exists(model_path) and os.path.exists(scaler_path):
            self.load_model()
        else:
            logger.info("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é")
            self._create_new_model()
    
    def _create_new_model(self):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é LSTM –º–æ–¥–µ–ª—å"""
        if not PYTORCH_AVAILABLE:
            return
        
        try:
            sequence_length = self.config['sequence_length']
            n_features = len(self.config['features'])
            
            # –°–æ–∑–¥–∞–µ–º PyTorch –º–æ–¥–µ–ª—å
            self.model = LSTMModel(input_size=n_features)
            self.model.to(DEVICE)
            self.model.eval()  # –†–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        except NameError as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}. PyTorch –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
            return
        
        # –°–æ–∑–¥–∞–µ–º scaler
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        
        logger.info("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å")
        logger.info(f"–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {sequence_length} —Å–≤–µ—á–µ–π ‚Üí {n_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    
    def prepare_features(self, candles: List[Dict]) -> np.ndarray:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Å–≤–µ—á–µ–π –¥–ª—è –º–æ–¥–µ–ª–∏
        
        Args:
            candles: –°–ø–∏—Å–æ–∫ —Å–≤–µ—á–µ–π —Å OHLCV –¥–∞–Ω–Ω—ã–º–∏
        
        Returns:
            –ú–∞—Å—Å–∏–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
        """
        if len(candles) < self.config['sequence_length']:
            logger.debug(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–≤–µ—á–µ–π: {len(candles)} < {self.config['sequence_length']}")
            return None
        
        df = pd.DataFrame(candles)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = []
        for feature in self.config['features']:
            if feature in df.columns:
                features.append(df[feature].values)
            else:
                # –ï—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞ –Ω–µ—Ç, –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                logger.warning(f"–ü—Ä–∏–∑–Ω–∞–∫ {feature} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö")
                features.append(np.zeros(len(df)))
        
        # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å (samples, features)
        features = np.array(features).T
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ sequence_length —Å–≤–µ—á–µ–π
        features = features[-self.config['sequence_length']:]
        
        return features.astype(np.float32)
    
    def predict(
        self,
        candles: List[Dict],
        current_price: float
    ) -> Optional[Dict]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –¥–≤–∏–∂–µ–Ω–∏–µ —Ü–µ–Ω—ã
        
        Args:
            candles: –ò—Å—Ç–æ—Ä–∏—è —Å–≤–µ—á–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            current_price: –¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞
        
        Returns:
            {
                'direction': 1 (–≤–≤–µ—Ä—Ö) –∏–ª–∏ -1 (–≤–Ω–∏–∑),
                'change_percent': –æ–∂–∏–¥–∞–µ–º–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤ %,
                'confidence': —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (0-100),
                'predicted_price': –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞,
                'horizon_hours': –≥–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —á–∞—Å–∞—Ö
            }
        """
        if not PYTORCH_AVAILABLE or self.model is None:
            return None
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = self.prepare_features(candles)
            if features is None:
                return None
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
            try:
                features_scaled = self.scaler.transform(features)
            except NotFittedError:
                logger.error("Scaler –Ω–µ –æ–±—É—á–µ–Ω. –í—ã–ø–æ–ª–Ω–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏")
                return None
            except Exception as transform_error:
                logger.error(f"–û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {transform_error}")
                return None
            
            # –î–æ–±–∞–≤–ª—è–µ–º batch dimension –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ tensor
            features_tensor = torch.FloatTensor(features_scaled).unsqueeze(0).to(DEVICE)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            if GPU_AVAILABLE and DEVICE and features_tensor.device.type == 'cuda':
                logger.debug(f"üöÄ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –Ω–∞ GPU: {DEVICE}")
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            self.model.eval()
            with torch.no_grad():
                prediction = self.model(features_tensor)
                # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º GPU –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–Ω–æ—Å–æ–º –Ω–∞ CPU
                if GPU_AVAILABLE and DEVICE and features_tensor.device.type == 'cuda':
                    torch.cuda.synchronize()
                prediction = prediction.cpu().numpy()[0]
            
            # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            direction_raw = prediction[0]  # -1 –¥–æ 1
            change_percent = prediction[1]  # % –∏–∑–º–µ–Ω–µ–Ω–∏—è
            confidence = prediction[2]  # 0-1
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            direction = 1 if direction_raw > 0 else -1
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence = min(max(abs(confidence) * 100, 0), 100)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—É—é —Ü–µ–Ω—É
            predicted_price = current_price * (1 + change_percent / 100)
            
            result = {
                'direction': direction,
                'change_percent': float(change_percent),
                'confidence': float(confidence),
                'predicted_price': float(predicted_price),
                'horizon_hours': self.config['prediction_horizon'],
                'current_price': current_price
            }
            
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return None
    
    def train(
        self,
        training_data: List[Tuple[np.ndarray, np.ndarray]],
        validation_split: float = 0.2,
        epochs: int = 50,
        batch_size: int = 32,
        learning_rate: float = 0.001
    ) -> Dict:
        """
        –û–±—É—á–∞–µ—Ç LSTM –º–æ–¥–µ–ª—å
        
        Args:
            training_data: –°–ø–∏—Å–æ–∫ (X, y) –≥–¥–µ X - –ø—Ä–∏–∑–Ω–∞–∫–∏, y - —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            validation_split: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        
        Returns:
            –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        """
        if not PYTORCH_AVAILABLE or self.model is None:
            return {'error': 'PyTorch unavailable'}

        if not training_data:
            logger.error("–ü—É—Å—Ç–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return {'error': 'No training data provided'}
        
        try:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            X_list, y_list = zip(*training_data)
            X = np.array(X_list)
            y = np.array(y_list)

            if X.ndim != 3:
                raise ValueError(f"Training data X –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (samples, seq_len, features), –ø–æ–ª—É—á–µ–Ω–æ: {X.shape}")

            if np.isnan(X).any() or np.isinf(X).any():
                logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN/Inf –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–º–µ–Ω—É –Ω–∞ –Ω—É–ª–∏")
                X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if X.shape[-1] != len(self.config['features']):
                logger.warning(
                    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (%s) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π (%s). –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥.",
                    X.shape[-1], len(self.config['features'])
                )
                self.config['features'] = [f'feature_{i}' for i in range(X.shape[-1])]
                # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –Ω–æ–≤—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                self._create_new_model()

            # –û–±—É—á–∞–µ–º scaler –Ω–∞ –≤—Å–µ–º –º–∞—Å—Å–∏–≤–µ
            if self.scaler is None:
                self.scaler = MinMaxScaler(feature_range=(0, 1))

            flat_X = X.reshape(-1, X.shape[-1])
            self.scaler.fit(flat_X)
            X_scaled = self.scaler.transform(flat_X).reshape(X.shape).astype(np.float32)
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train –∏ validation
            split_idx = int(len(X_scaled) * (1 - validation_split))
            X_train, X_val = X_scaled[:split_idx], X_scaled[split_idx:]
            y_train, y_val = y[:split_idx], y[split_idx:]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ PyTorch tensors
            X_train_tensor = torch.FloatTensor(X_train).to(DEVICE)
            y_train_tensor = torch.FloatTensor(y_train).to(DEVICE)
            X_val_tensor = torch.FloatTensor(X_val).to(DEVICE)
            y_val_tensor = torch.FloatTensor(y_val).to(DEVICE)
            
            # –°–æ–∑–¥–∞–µ–º DataLoader
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å
            optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
            criterion = nn.MSELoss()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –ª–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
            device_info = "CPU"
            if PYTORCH_AVAILABLE and GPU_AVAILABLE and DEVICE:
                device_info = f"GPU NVIDIA ({DEVICE})"
                logger.info(f"üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {device_info}")
            else:
                logger.info(f"üíª –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {device_info}")
            
            logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è: {len(X_train)} –æ–±—Ä–∞–∑—Ü–æ–≤ (train), {len(X_val)} –æ–±—Ä–∞–∑—Ü–æ–≤ (val)")
            logger.info(f"–§–æ—Ä–º–∞ X: {X.shape}, —Ñ–æ—Ä–º–∞ y: {y.shape}")
            
            # –û–±—É—á–µ–Ω–∏–µ
            self.model.train()
            best_val_loss = float('inf')
            patience = 10
            patience_counter = 0
            history = {'loss': [], 'val_loss': []}
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
            if GPU_AVAILABLE and DEVICE:
                logger.info(f"üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ GPU: {DEVICE}")
                memory_before = torch.cuda.memory_allocated(0) / 1024**2
                memory_reserved = torch.cuda.memory_reserved(0) / 1024**2
                logger.info(f"üìä –ü–∞–º—è—Ç—å GPU –¥–æ –æ–±—É—á–µ–Ω–∏—è: {memory_before:.2f} MB (–≤—ã–¥–µ–ª–µ–Ω–æ) / {memory_reserved:.2f} MB (–∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ)")
            
            for epoch in range(epochs):
                # –û–±—É—á–µ–Ω–∏–µ
                epoch_loss = 0.0
                epoch_start_time = time.time()
                
                for batch_idx, (batch_X, batch_y) in enumerate(train_loader):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
                    if GPU_AVAILABLE and DEVICE:
                        if batch_X.device.type != 'cuda':
                            logger.warning(f"‚ö†Ô∏è Batch {batch_idx}: –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞ GPU! –ü–µ—Ä–µ–º–µ—â–∞—é...")
                            batch_X = batch_X.to(DEVICE)
                            batch_y = batch_y.to(DEVICE)
                    
                    optimizer.zero_grad()
                    outputs = self.model(batch_X)
                    loss = criterion(outputs, batch_y)
                    loss.backward()
                    optimizer.step()
                    epoch_loss += loss.item()
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –∫–∞–∂–¥—ã–µ 10 –±–∞—Ç—á–µ–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3 —ç–ø–æ—Ö)
                    if GPU_AVAILABLE and DEVICE and epoch < 3 and batch_idx % 10 == 0:
                        memory_used = torch.cuda.memory_allocated(0) / 1024**2
                        logger.debug(f"üìä –≠–ø–æ—Ö–∞ {epoch+1}/{epochs}, –ë–∞—Ç—á {batch_idx}: GPU –ø–∞–º—è—Ç—å = {memory_used:.2f} MB, Loss = {loss.item():.6f}")
                
                # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º GPU –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏
                if GPU_AVAILABLE and DEVICE:
                    torch.cuda.synchronize()
                    memory_after = torch.cuda.memory_allocated(0) / 1024**2
                    if epoch % 5 == 0 or epoch == 0:  # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö
                        logger.info(f"üìä –≠–ø–æ—Ö–∞ {epoch+1}/{epochs}: Loss={avg_train_loss:.6f}, GPU –ø–∞–º—è—Ç—å={memory_after:.2f} MB")
                
                epoch_time = time.time() - epoch_start_time
                avg_train_loss = epoch_loss / len(train_loader)
                
                avg_train_loss = epoch_loss / len(train_loader)
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è
                self.model.eval()
                with torch.no_grad():
                    val_outputs = self.model(X_val_tensor)
                    val_loss = criterion(val_outputs, y_val_tensor).item()
                
                self.model.train()
                
                history['loss'].append(avg_train_loss)
                history['val_loss'].append(val_loss)
                
                # Early stopping
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                    torch.save(self.model.state_dict(), self.model_path + '.best')
                else:
                    patience_counter += 1
                
                # Learning rate scheduling
                if patience_counter >= 5:
                    for param_group in optimizer.param_groups:
                        param_group['lr'] *= 0.5
                        if param_group['lr'] < 0.00001:
                            param_group['lr'] = 0.00001
                
                if (epoch + 1) % 10 == 0 or epoch == 0:
                    logger.info(f"Epoch {epoch+1}/{epochs} - Loss: {avg_train_loss:.6f}, Val Loss: {val_loss:.6f}")
                
                if patience_counter >= patience:
                    logger.info(f"Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                    self.model.load_state_dict(torch.load(self.model_path + '.best'))
                    break
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            self.config['trained_at'] = datetime.now().isoformat()
            self.config['training_samples'] = len(X)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self.save_model()
            
            logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
            
            return {
                'success': True,
                'final_loss': float(history['loss'][-1]),
                'final_val_loss': float(history['val_loss'][-1]),
                'epochs_trained': len(history['loss']),
                'training_samples': len(X)
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}", exc_info=True)
            return {'error': str(e)}
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å, scaler –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
        if not PYTORCH_AVAILABLE or self.model is None:
            return
        
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            torch.save(self.model.state_dict(), self.model_path)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
            with open(self.scaler_path, 'wb') as f:
                pickle.dump(self.scaler, f)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            with open(self.config_path, 'w') as f:
                json.dump(self.config, f, indent=2)
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {self.model_path}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å, scaler –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"""
        if not PYTORCH_AVAILABLE:
            return
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r') as f:
                    loaded_config = json.load(f)
                    self.config.update(loaded_config)
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            n_features = len(self.config['features'])
            self.model = LSTMModel(input_size=n_features)
            self.model.to(DEVICE)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
            self.model.load_state_dict(torch.load(self.model_path, map_location=DEVICE))
            self.model.eval()
        except NameError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}. PyTorch –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
            self._create_new_model()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º scaler
            with open(self.scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {self.model_path}")
            logger.info(f"–û–±—É—á–µ–Ω–∞: {self.config.get('trained_at', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            logger.info(f"–û–±—Ä–∞–∑—Ü–æ–≤: {self.config.get('training_samples', 0)}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self._create_new_model()
    
    def get_status(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏"""
        if not PYTORCH_AVAILABLE:
            return {
                'available': False,
                'error': 'PyTorch not installed'
            }
        
        is_trained = (
            self.model is not None and
            os.path.exists(self.model_path) and
            self.config.get('training_samples', 0) > 0
        )
        
        status = {
            'available': True,
            'trained': is_trained,
            'model_path': self.model_path,
            'sequence_length': self.config['sequence_length'],
            'prediction_horizon': self.config['prediction_horizon'],
            'trained_at': self.config.get('trained_at'),
            'training_samples': self.config.get('training_samples', 0),
            'features': self.config['features'],
            'framework': 'PyTorch'
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU
        if PYTORCH_AVAILABLE:
            status['gpu_available'] = GPU_AVAILABLE
            status['device'] = str(DEVICE) if DEVICE else 'cpu'
            if GPU_AVAILABLE and DEVICE:
                try:
                    import torch
                    status['gpu_name'] = torch.cuda.get_device_name(0) if torch.cuda.is_available() else None
                    status['cuda_version'] = torch.version.cuda if torch.cuda.is_available() else None
                except:
                    pass
        
        return status
