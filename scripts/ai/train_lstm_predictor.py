"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LSTM –º–æ–¥–µ–ª–∏ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LSTM
–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –¥–≤–∏–∂–µ–Ω–∏–µ —Ü–µ–Ω—ã –Ω–∞ 6 —á–∞—Å–æ–≤ –≤–ø–µ—Ä–µ–¥.
"""

import os
import sys
import argparse
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from bot_engine.ai.lstm_predictor import LSTMPredictor, TENSORFLOW_AVAILABLE
from utils.rsi_calculator import calculate_rsi


def calculate_ema(prices: np.ndarray, period: int) -> np.ndarray:
    """–í—ã—á–∏—Å–ª—è–µ—Ç EMA"""
    ema = np.zeros_like(prices)
    ema[0] = prices[0]
    multiplier = 2 / (period + 1)
    
    for i in range(1, len(prices)):
        ema[i] = (prices[i] - ema[i-1]) * multiplier + ema[i-1]
    
    return ema


def prepare_training_data(
    csv_file: str,
    sequence_length: int = 60,
    prediction_horizon: int = 1
) -> list:
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ CSV —Ñ–∞–π–ª–∞
    
    Args:
        csv_file: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        sequence_length: –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (60 —Å–≤–µ—á–µ–π)
        prediction_horizon: –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (1 —Å–≤–µ—á–∞ = 6 —á–∞—Å–æ–≤)
    
    Returns:
        –°–ø–∏—Å–æ–∫ (X, y) –ø–∞—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    """
    print(f"  –ó–∞–≥—Ä—É–∑–∫–∞: {csv_file}")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = pd.read_csv(csv_file)
        
        if len(df) < sequence_length + prediction_horizon + 20:
            print(f"  ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(df)} —Å–≤–µ—á–µ–π)")
            return []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_cols = ['close', 'volume', 'high', 'low']
        if not all(col in df.columns for col in required_cols):
            print(f"  ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏")
            return []
        
        # –í—ã—á–∏—Å–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        print("  üìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # RSI
        df['rsi'] = calculate_rsi(df['close'].values, period=14)
        
        # EMA
        df['ema_fast'] = calculate_ema(df['close'].values, period=12)
        df['ema_slow'] = calculate_ema(df['close'].values, period=26)
        
        # –£–¥–∞–ª—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è
        df = df.dropna()
        
        if len(df) < sequence_length + prediction_horizon + 20:
            print(f"  ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ ({len(df)} —Å–≤–µ—á–µ–π)")
            return []
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = ['close', 'volume', 'high', 'low', 'rsi', 'ema_fast', 'ema_slow']
        data = df[features].values
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ (MinMaxScaler –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω –≤ LSTMPredictor)
        # –ó–¥–µ—Å—å –º—ã –ø—Ä–æ—Å—Ç–æ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        
        training_samples = []
        
        # –°–æ–∑–¥–∞–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ
        for i in range(len(data) - sequence_length - prediction_horizon):
            # –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (60 —Å–≤–µ—á–µ–π)
            X = data[i:i + sequence_length]
            
            # –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (—Å–ª–µ–¥—É—é—â–∞—è —Å–≤–µ—á–∞ —á–µ—Ä–µ–∑ prediction_horizon)
            current_close = data[i + sequence_length - 1, 0]  # close
            future_close = data[i + sequence_length + prediction_horizon - 1, 0]
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:
            # 1. –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: 1 (–≤–≤–µ—Ä—Ö) –∏–ª–∏ -1 (–≤–Ω–∏–∑)
            direction = 1.0 if future_close > current_close else -1.0
            
            # 2. –ò–∑–º–µ–Ω–µ–Ω–∏–µ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            change_percent = ((future_close - current_close) / current_close) * 100
            
            # 3. –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ–ª–∏—á–∏–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è)
            confidence = min(abs(change_percent) / 10, 1.0)  # 0-1
            
            y = np.array([direction, change_percent, confidence])
            
            training_samples.append((X, y))
        
        print(f"  ‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {len(training_samples)}")
        return training_samples
        
    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {e}")
        return []


def load_all_historical_data(
    data_dir: str = "data/ai/historical",
    max_coins: int = 0,
    sequence_length: int = 60
) -> list:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    
    Args:
        data_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å CSV —Ñ–∞–π–ª–∞–º–∏
        max_coins: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–Ω–µ—Ç (0 = –≤—Å–µ)
        sequence_length: –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    
    Returns:
        –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –æ–±—É—á–∞—é—â–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
    """
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {data_dir}")
        return []
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö CSV —Ñ–∞–π–ª–æ–≤
    csv_files = sorted(data_path.glob("*.csv"))
    
    if max_coins > 0:
        csv_files = csv_files[:max_coins]
    
    print(f"\nüìä –ù–∞–π–¥–µ–Ω–æ CSV —Ñ–∞–π–ª–æ–≤: {len(csv_files)}")
    print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    print("=" * 60)
    
    all_training_data = []
    successful = 0
    failed = 0
    
    for i, csv_file in enumerate(csv_files, 1):
        print(f"\n[{i}/{len(csv_files)}] {csv_file.name}")
        
        training_data = prepare_training_data(
            str(csv_file),
            sequence_length=sequence_length
        )
        
        if training_data:
            all_training_data.extend(training_data)
            successful += 1
        else:
            failed += 1
    
    print("\n" + "=" * 60)
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful} –º–æ–Ω–µ—Ç")
    print(f"‚ùå –û—à–∏–±–æ–∫: {failed} –º–æ–Ω–µ—Ç")
    print(f"üì¶ –í—Å–µ–≥–æ –æ–±—É—á–∞—é—â–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {len(all_training_data)}")
    
    return all_training_data


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    parser = argparse.ArgumentParser(description='–û–±—É—á–µ–Ω–∏–µ LSTM –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞')
    parser.add_argument('--coins', type=int, default=0, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–Ω–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (0 = –≤—Å–µ)')
    parser.add_argument('--epochs', type=int, default=50, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--batch-size', type=int, default=32, help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞')
    parser.add_argument('--sequence-length', type=int, default=60, help='–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏')
    args = parser.parse_args()
    
    print("=" * 60)
    print("üß† –û–ë–£–ß–ï–ù–ò–ï LSTM PREDICTOR")
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º TensorFlow
    if not TENSORFLOW_AVAILABLE:
        print("‚ùå TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install tensorflow")
        return 1
    
    print(f"\n‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"  –ú–æ–Ω–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {'–≤—Å–µ' if args.coins == 0 else args.coins}")
    print(f"  –≠–ø–æ—Ö: {args.epochs}")
    print(f"  –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {args.batch_size}")
    print(f"  –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {args.sequence_length} —Å–≤–µ—á–µ–π")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
    training_data = load_all_historical_data(
        max_coins=args.coins,
        sequence_length=args.sequence_length
    )
    
    if not training_data:
        print("\n‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/ai/collect_historical_data.py")
        return 1
    
    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    print("\n" + "=" * 60)
    print("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 60)
    
    predictor = LSTMPredictor()
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è scaler
    print("\nüìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏...")
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ X –¥–ª—è –æ–±—É—á–µ–Ω–∏—è scaler
    X_list = [x for x, _ in training_data]
    X_all = np.vstack(X_list)
    
    # –û–±—É—á–∞–µ–º scaler
    predictor.scaler.fit(X_all.reshape(-1, X_all.shape[-1]))
    print("‚úÖ Scaler –æ–±—É—á–µ–Ω")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
    print("üìä –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
    normalized_data = []
    for X, y in training_data:
        X_scaled = predictor.scaler.transform(X)
        normalized_data.append((X_scaled, y))
    
    print(f"‚úÖ –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: {len(normalized_data)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    print("\nüß† –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")
    result = predictor.train(
        training_data=normalized_data,
        validation_split=0.2,
        epochs=args.epochs,
        batch_size=args.batch_size
    )
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "=" * 60)
    if result.get('success'):
        print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        print("=" * 60)
        print(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ (train): {result['final_loss']:.6f}")
        print(f"üìä –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ (val): {result['final_val_loss']:.6f}")
        print(f"üìà –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–æ: {result['epochs_trained']}")
        print(f"üì¶ –û–±—É—á–∞—é—â–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {result['training_samples']}")
        print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: data/ai/models/lstm_predictor.h5")
        return 0
    else:
        print("‚ùå –û–®–ò–ë–ö–ê –û–ë–£–ß–ï–ù–ò–Ø!")
        print("=" * 60)
        print(f"Error: {result.get('error', 'Unknown')}")
        return 1


if __name__ == "__main__":
    exit(main())

